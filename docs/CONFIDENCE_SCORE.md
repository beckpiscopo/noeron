# Confidence Score

## Overview

The confidence score (0-100%) represents **how strongly a claim needs scientific backing**. It is not a measure of accuracy or certainty, but rather an assessment of how important it is to verify the claim with evidence.

## How It's Calculated

### Primary Score: Gemini LLM Assessment

The confidence score is generated by **Gemini 3 Pro** during the initial claim extraction phase. When analyzing podcast transcripts, Gemini evaluates each claim and assigns a score based on:

- **Claim extraordinariness**: Counter-intuitive or surprising claims receive higher scores
- **Speaker stance**: Assertions vs hypotheses vs predictions
- **Need for evidence**: How much the claim would benefit from scientific backing
- **Specificity**: Specific quantitative claims vs general statements

**Prompt excerpt** (from `scripts/context_card_builder.py`):
```
Assign `confidence_score` (0-1) for how strongly the claim needs backing
```

### Score Normalization

- Raw scores from Gemini are clamped to [0.0, 1.0]
- If Gemini doesn't provide a score, defaults to 1.0
- Displayed in the UI as a percentage (0-100%)

## Secondary Verification (Optional)

A separate verification agent (`agents/verification_agent.py`) can calculate an additional `verification_confidence` score using:

| Factor | Weight | Description |
|--------|--------|-------------|
| Temporal validation | 20% | Paper publication date vs podcast date |
| Citation network | 40% | How many papers in the corpus cite this source |
| Cross-reference | 40% | Citation overlap with similar claims |

## Interpretation Guide

| Score | Meaning |
|-------|---------|
| 80-100% | Strong claim that significantly benefits from evidence |
| 50-79% | Moderate claim that would benefit from supporting research |
| 0-49% | Common knowledge or well-established fact |

## Technical Details

### Data Flow

1. **Extraction**: Gemini analyzes transcript segment â†’ outputs `confidence_score` (0-1)
2. **Storage**: Score saved to Supabase (`claims` table, `confidence_score REAL`)
3. **Aggregation**: Backend calculates average across RAG results
4. **Display**: Frontend renders as percentage

### Code References

- **Gemini prompt**: `scripts/context_card_builder.py` (lines 125-150)
- **Normalization**: `scripts/context_card_builder.py` (`_normalize_confidence()`)
- **Backend usage**: `src/bioelectricity_research/server.py` (lines 1261, 1344-1345)
- **Frontend display**: `frontend/components/listening-view.tsx`
- **Database schema**: `supabase/schema.sql` (line 60)

### Configuration

- **Model**: Set via `GEMINI_MODEL` env var (default: `gemini-3-pro-preview`)
- **Default score**: 1.0 if parsing fails
